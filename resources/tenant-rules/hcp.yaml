# ============================================================================
# WARNING: This file is auto-generated from resources/tenant-rules/hcp/*.yaml
# DO NOT EDIT THIS FILE DIRECTLY!
#
# To modify HCP tenant rules:
#   1. Edit the appropriate file in resources/tenant-rules/hcp/
#   2. Run: make hcp-rules
#   3. Commit both the source file and this generated file
#
# Source files:
#   - hcp/api-server.yaml      (kube-apiserver, openshift-apiserver, SLOs)
#   - hcp/audit.yaml           (audit webhook CloudWatch)
#   - hcp/billing.yaml         (billing metrics)
#   - hcp/cluster-operators.yaml (ClusterOperator health)
#   - hcp/control-plane.yaml   (etcd, kube-controller-manager, kube-scheduler)
#   - hcp/nodes.yaml           (node health, nodepool, autoscaler)
#   - hcp/oauth.yaml           (OAuth service health)
#   - hcp/observability.yaml   (watchdog, prometheus targets)
#   - hcp/splunk.yaml          (SAE deployment)
# ============================================================================
apiVersion: template.openshift.io/v1
kind: Template
metadata:
  name: rhobs-hcp-rules
  annotations:
    description: Template for deploying HCP rules to RHOBS
parameters:
  - name: NAMESPACE
    description: Namespace to deploy the rules to
    required: true
  - name: TENANT
    description: Tenant to deploy the rules to
    required: true
objects:
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: audit-webhook-error
      namespace: ${NAMESPACE}
      labels:
        operator.thanos.io/prometheus-rule: "true"
        operator.thanos.io/tenant: ${TENANT}
    spec:
      groups:
        - name: AuditWebhookCloudWatchErrors
          interval: 60s
          rules:
            - alert: AuditWebhookIncorrectCloudwatchConfiguration
              annotations:
                description: The audit webhook cloudwatch configuration for {{ $labels.namespace }} has been invalid for 5 minutes.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/v4/howto/customer-cw-audit-forwarding-for-hcp.md
                summary: Audit webhook cloudwatch configuration invalid
              expr: sum by (region, env, _mc_id, namespace, _id) (splunkforwarder_audit_filter_cloudwatch_configuration_invalid{}) > 0
              for: 5m
              labels:
                severity: warning
                otel_collect: "true"
            - alert: AuditWebhookCloudWatchErrors
              annotations:
                description: The audit webhook cloudwatch integration for {{ $labels.namespace }} is experiencing problems.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/hypershift/alerts/AuditWebhookCloudWatchErrors.md
                summary: Audit webhook cloudwatch error
              expr: |
                (sum(splunkforwarder_audit_filter_cloudwatch_enabled) by (_id, _mc_id, namespace, region, env))
                * on (_id, _mc_id, namespace, region, env)
                (sum(rate(splunkforwarder_audit_filter_cloudwatch_errors_total[10m])) by (_id, _mc_id, namespace, region, env)) > 0
              for: 10m
              labels:
                severity: warning
                otel_collect: "true"
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: billing-rules
      namespace: ${NAMESPACE}
      labels:
        operator.thanos.io/prometheus-rule: "true"
        operator.thanos.io/tenant: ${TENANT}
    spec:
      groups:
        - name: BillingRules
          interval: 30s
          rules:
            - alert: BillingMetricMissing
              annotations:
                description: Recording rule hostedcluster:hypershift_cluster_vcpus:max is not defined for cluster {{ $labels._id }}.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/BillingMetricMissing.md
                summary: Recording rule used for the billing is not defined
              expr: |
                hypershift_cluster_vcpus_computation_error or (hypershift_cluster_silence_alerts unless on(_id, _mc_id) hostedcluster:hypershift_cluster_vcpus:max)
              for: 30m
              labels:
                severity: critical
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: nodepool-failure
      namespace: ${NAMESPACE}
      labels:
        operator.thanos.io/prometheus-rule: "true"
        operator.thanos.io/tenant: ${TENANT}
    spec:
      groups:
        - name: NodePoolFailing
          interval: 1m
          rules:
            - alert: NodePoolFailing
              expr: hypershift_nodepools:replicas_failure > 0
              for: 15m
              labels:
                severity: warning
              annotations:
                message: '{{ $labels.nodepool_name }} nodepool on {{ $labels._id }} is not creating nodes'
                runbook_url: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/NodePoolFailing.md
                summary: NodePool is not creating nodes
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: nodes-need-upscale
      namespace: ${NAMESPACE}
      labels:
        operator.thanos.io/prometheus-rule: "true"
        operator.thanos.io/tenant: ${TENANT}
    spec:
      groups:
        - name: NodesNeedUpscale
          interval: 1m
          rules:
            - alert: NodesNeedUpscale
              expr: hypershift_cluster:expected_total_nodes{replicas_failure_all_nodepools="0"} > hypershift_cluster:current_ready_nodes{replicas_failure_all_nodepools="0"} + 1
              for: 60m
              labels:
                severity: warning
              annotations:
                message: HCP cluster {{ $labels._id }} is short on nodes
                runbook_url: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/NodesNeedUpscale.md
                summary: Nodes need upscaling
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: cluster-autoscaler-rules
      namespace: ${NAMESPACE}
      labels:
        operator.thanos.io/prometheus-rule: "true"
        operator.thanos.io/tenant: ${TENANT}
    spec:
      groups:
        - name: cluster-autoscaler.rules
          interval: 1m
          rules:
            - alert: ClusterAutoscalerDown
              annotations:
                description: There are 0 Ready 'cluster-autoscaler' Pods for {{ $labels.namespace }}.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/hypershift/alerts/ClusterAutoscalerAlert.md
                summary: No Ready cluster-autoscaler pods.
              labels:
                severity: critical
              expr: cluster_autoscaler:pod_status_ready == 0
              for: 15m
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: nodes-rules
      namespace: ${NAMESPACE}
      labels:
        operator.thanos.io/prometheus-rule: "true"
        operator.thanos.io/tenant: ${TENANT}
    spec:
      groups:
        - name: NodesRules
          interval: 30s
          rules:
            - alert: NodeHighResourceUsage
              expr: hypershift_node:high_usage
              for: 30m
              labels:
                severity: warning
              annotations:
                summary: High {{ $labels.resource }} usage on node {{ $labels.node }}
                runbook_url: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/NodeHighResourceUsage.md
                description: 'Node {{ $labels.node }} has {{ $labels.resource }} usage above {{ $labels.threshold }} (current value: {{ $value | humanizePercentage }}) for more than 30 minutes.'
            - alert: NodeNotReady
              expr: hypershift_node:ready == 0
              for: 30m
              labels:
                severity: warning
              annotations:
                summary: Node {{ $labels.node }} is not ready
                runbook_url: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/NodeNotReady.md
                description: Node {{ $labels.node }} has not been ready for more than 30 minutes.
            - alert: NodeInBadCondition
              expr: hypershift_node:in_bad_condition == 1
              for: 30m
              labels:
                severity: warning
              annotations:
                summary: Node {{ $labels.node }} is in bad condition
                runbook_url: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/NodeInBadCondition.md
                description: Condition {{ $labels.condition }} has been triggering on node {{ $labels.node }} for more than 30 minutes.
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: sre-node-not-joining-nodepool-sre-actionable-rules
      namespace: ${NAMESPACE}
      labels:
        operator.thanos.io/prometheus-rule: "true"
        operator.thanos.io/tenant: ${TENANT}
    spec:
      groups:
        - name: sre-NodepoolFailureSRE
          interval: 30s
          rules:
            - alert: NodepoolFailureSRE
              annotations:
                description: One or more nodepool from the cluster are having issue while adding node(s)
                runbook_url: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/NodepoolFailureSRE.md
                summary: HCP Cluster nodepool having issues while adding nodes
              expr: sre:nodepool:provisioning_failure_notify and on (exported_namespace, _mc_id) (sre:nodepool:all_components_available==0 or sre:nodepool:invalid_payload_nodepool_provision_failure)
              for: 1h
              labels:
                otel_collect: "true"
                severity: warning
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: sre-nodes-need-upscale-rules
      namespace: ${NAMESPACE}
      labels:
        operator.thanos.io/prometheus-rule: "true"
        operator.thanos.io/tenant: ${TENANT}
    spec:
      groups:
        - name: sre-RequestServingNodesNeedUpscale
          interval: 30s
          rules:
            - alert: RequestServingNodesNeedUpscale
              annotations:
                description: The cluster's request serving nodes have been undersized for 1 hour and need to be assigned to a bigger request serving node pair.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/hypershift/alerts/RequestServingNodesNeedUpscale.md
                summary: HCP Request Serving Nodes Need Upsizing
              expr: count(sre:node_request_serving:excessive_consumption_cpu or sre:node_request_serving:excessive_consumption_memory) by (request_node, _mc_id)
              for: 1h
              labels:
                severity: critical
                otel_collect: "true"
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: oauth-service-health
      namespace: ${NAMESPACE}
      labels:
        operator.thanos.io/prometheus-rule: "true"
        operator.thanos.io/tenant: ${TENANT}
    spec:
      groups:
        - name: OauthServiceRules
          interval: 30s
          rules:
            - alert: OauthServiceDeploymentDegraded
              annotations:
                description: An Oauth Service deployment does not have the expected number of replicas.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/OauthServiceDeploymentDegraded.md
                summary: An Oauth Service deployment does not have the expected number of available replicas.
              expr: |
                sre:oauth:deployment_pods_unavailable > 0
              for: 10m
              labels:
                severity: warning
                otel_collect: "true"
            - alert: OauthServiceDeploymentDown
              annotations:
                description: There are no ready Oauth Service pods in the HCP Namespace.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/OauthServiceDeploymentDown.md
                summary: There are no ready Oauth Service pods in the HCP Namespace.
              labels:
                severity: warning
                otel_collect: "true"
              expr: sre:sre:oauth:pod_status_ready == 0
              for: 15m
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: watchdog
      namespace: ${NAMESPACE}
      labels:
        operator.thanos.io/prometheus-rule: "true"
        operator.thanos.io/tenant: ${TENANT}
    spec:
      groups:
        - name: watchdog
          interval: 30s
          rules:
            - alert: DeadMansSnitch
              annotations:
                summary: No `watchdog` heartbeat for {{ $labels._id }} detected
                description: No `watchdog` heartbeat for {{ $labels._id }} detected. This means the alerting stack is not functioning properly and alerts may not fire.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/DeadMansSnitch.md
              expr: sum by (_id, _mc_id, region, sector) (vector(1)) unless on (_id) (watchdog)
              for: 15m
              labels:
                severity: critical
            - record: watchdog
              expr: vector(1)
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: sre-prometheus-target-alerting
      namespace: ${NAMESPACE}
      labels:
        operator.thanos.io/prometheus-rule: "true"
        operator.thanos.io/tenant: ${TENANT}
    spec:
      groups:
        - name: sre-prometheus-target-alerting
          interval: 1m
          rules:
            - alert: HCPPrometheusPodMonitorDown
              expr: sre:hcp_podmonitor_up:sum{name!~"karpenter"} == 0
              for: 30m
              labels:
                severity: critical
              annotations:
                summary: PodMonitor {{ $labels.name }} in namespace {{ $labels.namespace }} is down.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/HCPPrometheusPodMonitorDown.md
                description: PodMonitor {{ $labels.name }} in namespace {{ $labels.namespace }} has been down for 30m.
            - alert: HCPPrometheusServiceMonitorDown
              expr: (sre:hcp_servicemonitor_up:sum{name!~"node-tuning-operator|ovnkube-control-plane|cluster-version-operator"} == 0) unless on (_id) hypershift_cluster_waiting_initial_avaibility_duration_seconds
              for: 30m
              labels:
                severity: critical
              annotations:
                summary: ServiceMonitor {{ $labels.name }} in namespace {{ $labels.namespace }} is down.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/HCPPrometheusServiceMonitorDown.md
                description: ServiceMonitor {{ $labels.name }} in namespace {{ $labels.namespace }} has been down for 30m.
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: cluster-operators
      namespace: ${NAMESPACE}
      labels:
        operator.thanos.io/prometheus-rule: "true"
        operator.thanos.io/tenant: ${TENANT}
    spec:
      groups:
        - name: cluster-operators
          interval: 60s
          rules:
            - alert: ClusterOperatorDegraded
              annotations:
                summary: Cluster operator is in degraded state.
                description: The {{ $labels.name }} operator is reporting a degraded state for {{ $labels._id }}.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/v4/alerts/cluster_operators/ClusterOperatorDegraded.md
              expr: (cluster_operator_conditions{condition="Degraded"} == 1) unless on (_id) hypershift_cluster_waiting_initial_avaibility_duration_seconds
              for: 60m
              labels:
                severity: warning
                otel_collect: "true"
            - alert: ClusterOperatorDown
              annotations:
                summary: Cluster operator is unavailable.
                description: The {{ $labels.name }} operator is unavailable for {{ $labels._id }}.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/v4/alerts/cluster_operators/ClusterOperatorDown.md
              expr: (cluster_operator_conditions{condition="Available"} == 0) unless on (_id) hypershift_cluster_waiting_initial_avaibility_duration_seconds
              for: 60m
              labels:
                severity: critical
                otel_collect: "true"
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: sre-etcd-rules
      namespace: ${NAMESPACE}
      labels:
        operator.thanos.io/prometheus-rule: "true"
        operator.thanos.io/tenant: ${TENANT}
    spec:
      groups:
        - name: sre-etcd-rules
          interval: 30s
          rules:
            - alert: etcdNoLeader
              annotations:
                summary: etcd cluster has no leader
                description: 'etcd cluster "{{ $labels.job }}": member {{ $labels.instance }} has no leader.'
                runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdNoLeader.md
              expr: etcd_server_has_leader{job=~".*etcd.*", _id!=""} == 0
              for: 1m
              labels:
                severity: critical
            - alert: etcdHighNumberOfLeaderChanges
              annotations:
                summary: etcd cluster has high number of leader changes.
                description: 'etcd cluster "{{ $labels.job }}": {{ $value }} average leader changes within the last 10 minutes. Frequent elections may be a sign of insufficient resources, high network latency, or disruptions by other components and should be investigated.'
              expr: avg by (_id, _mc_id) (changes(etcd_server_is_leader{_id!=""}[10m])) > 5
              for: 5m
              labels:
                severity: warning
            - alert: etcdDatabaseQuotaLowSpace
              annotations:
                description: 'etcd cluster for "{{ $labels._id }}": database size is 65% of the defined quota on etcd instance {{ $labels.instance }}, please defrag or increase the quota as the writes to etcd will be disabled when it is full.'
                summary: etcd cluster database is using >= 65% of the defined quota.
              expr: |
                (
                    max by (_id, _mc_id) (last_over_time(etcd_mvcc_db_total_size_in_bytes{job=~".*etcd.*"}[5m]))
                  /
                    max by (_id, _mc_id) (last_over_time(etcd_server_quota_backend_bytes{job=~".*etcd.*"}[5m]))
                )
                * 100 > 65
              for: 10m
              labels:
                severity: info
            - alert: etcdDatabaseQuotaLowSpace
              annotations:
                description: 'etcd cluster for "{{ $labels._id }}": database size is 75% of the defined quota on etcd instance {{ $labels.instance }}, please defrag or increase the quota as the writes to etcd will be disabled when it is full.'
                summary: etcd cluster database is using >= 75% of the defined quota.
              expr: |
                (
                    max by (_id, _mc_id) (last_over_time(etcd_mvcc_db_total_size_in_bytes{job=~".*etcd.*"}[5m]))
                  /
                    max by (_id, _mc_id) (last_over_time(etcd_server_quota_backend_bytes{job=~".*etcd.*"}[5m]))
                )
                * 100 > 75
              for: 10m
              labels:
                severity: warning
            - alert: etcdDatabaseQuotaLowSpace
              annotations:
                description: 'etcd cluster for "{{ $labels._id }}": database size is 85% of the defined quota on etcd instance {{ $labels.instance }}, please defrag or increase the quota as the writes to etcd will be disabled when it is full.'
                runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdDatabaseQuotaLowSpace.md
                summary: etcd cluster database is running full.
              expr: |
                (
                    max by (_id, _mc_id) (last_over_time(etcd_mvcc_db_total_size_in_bytes{job=~".*etcd.*"}[5m]))
                  /
                    max by (_id, _mc_id) (last_over_time(etcd_server_quota_backend_bytes{job=~".*etcd.*"}[5m]))
                )
                * 100 > 85
              for: 10m
              labels:
                severity: critical
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: kube-controller-manager
      namespace: ${NAMESPACE}
      labels:
        operator.thanos.io/prometheus-rule: "true"
        operator.thanos.io/tenant: ${TENANT}
    spec:
      groups:
        - name: osd-kube-controller-manager
          interval: 1m
          rules:
            - record: sre:kube_controller_manager:pod_status_ready
              expr: sum by (_id, _mc_id) (kube_pod_status_ready{pod=~"kube-controller-manager.*", namespace=~"ocm-.*-.*", condition="true"})
            - alert: KubeControllerManagerDown
              annotations:
                description: There are 0 Ready 'kube-controller-manager' Pods for {{ $labels.namespace }}.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/hypershift/alerts/KubeControllerManagerDown.md
                summary: No Ready kube-controller-manager pods.
              labels:
                severity: critical
              expr: sre:kube_controller_manager:pod_status_ready == 0
              for: 15m
            - alert: KubeControllerManagerDegraded
              annotations:
                description: There is not at least 2 Ready 'kube-controller-manager' Pods for {{ $labels.namespace }}.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/hypershift/alerts/KubeControllerManagerDown.md
                summary: Minimum Ready kube-controller-manager Pods not met.
              labels:
                severity: warning
              expr: sre:kube_controller_manager:pod_status_ready == 1
              for: 15m
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: kube-scheduler
      namespace: ${NAMESPACE}
      labels:
        operator.thanos.io/prometheus-rule: "true"
        operator.thanos.io/tenant: ${TENANT}
    spec:
      groups:
        - name: osd-kube-scheduler
          interval: 1m
          rules:
            - record: sre:kube_scheduler:pod_status_ready
              expr: sum by (_id, _mc_id) (kube_pod_status_ready{pod=~"kube-scheduler.*", namespace=~"ocm-.*-.*", condition="true"})
            - alert: KubeSchedulerDown
              annotations:
                description: There are 0 Ready 'kube-scheduler' Pods for {{ $labels.namespace }}.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/hypershift/alerts/KubeSchedulerDown.md
                summary: No Ready kube-scheduler pods.
              labels:
                severity: critical
              expr: sre:kube_scheduler:pod_status_ready == 0
              for: 15m
            - alert: KubeSchedulerDegraded
              annotations:
                description: There is not at least 2 Ready 'kube-scheduler' Pods for {{ $labels.namespace }}.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/hypershift/alerts/KubeSchedulerDown.md
                summary: Minimum Ready kube-scheduler Pods not met.
              labels:
                severity: warning
              expr: sre:kube_scheduler:pod_status_ready == 1
              for: 15m
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: kube-api-error-budget-burn
      namespace: ${NAMESPACE}
      labels:
        operator.thanos.io/prometheus-rule: "true"
        operator.thanos.io/tenant: ${TENANT}
    spec:
      groups:
        - name: kube-api-error-budget-burn
          interval: 30s
          rules:
            - record: sre:kube_apiserver:pod_status_ready
              expr: sum by (_id, _mc_id) (kube_pod_status_ready{pod=~"kube-apiserver.*", namespace=~"ocm-.*-.*", condition="true"})
            - record: sre:openshift_apiserver:pod_status_ready
              expr: sum by (_id, _mc_id) (kube_pod_status_ready{pod=~"openshift-apiserver.*", namespace=~"ocm-.*-.*", condition="true"})
            - record: sre:kube_apiserver:deployment_pods_unavailable
              expr: sum by (_id, _mc_id) (kube_deployment_status_replicas_unavailable{deployment="kube-apiserver", namespace=~"ocm-.*-.*"})
            - record: sre:openshift_apiserver:deployment_pods_unavailable
              expr: sum by (_id, _mc_id) (kube_deployment_status_replicas_unavailable{deployment="openshift-apiserver", namespace=~"ocm-.*-.*"})
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: api
      namespace: ${NAMESPACE}
      labels:
        operator.thanos.io/prometheus-rule: "true"
        operator.thanos.io/tenant: ${TENANT}
    spec:
      groups:
        - name: api-SLOs-probe
          interval: 30s
          rules:
            - alert: api-ErrorBudgetBurn
              annotations:
                message: High error budget burn for openshift api
                runbook_url: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/api-ErrorBudgetBurn.md
              expr: |-
                label_replace(
                        label_replace(
                            label_replace((

                    1-(sum by (probe_url, mc_name, _mc_id, sector, region, _id)(sum_over_time(probe_success{probe_url=~".*api.*"}[5m]))/ sum by (probe_url, mc_name, _mc_id, sector, region, _id)(count_over_time(probe_success{probe_url=~".*api.*"}[5m])))> (14.40*(1-0.990)) and sum by (probe_url, mc_name, _mc_id, sector, region, _id)(count_over_time(probe_success{probe_url=~".*api.*"}[5m])) > 5
                    and
                    1-(sum by (probe_url, mc_name, _mc_id, sector, region, _id)(sum_over_time(probe_success{probe_url=~".*api.*"}[1h]))/ sum by (probe_url, mc_name, _mc_id, sector, region, _id)(count_over_time(probe_success{probe_url=~".*api.*"}[1h])))> (14.40*(1-0.990)) and sum by (probe_url, mc_name, _mc_id, sector, region, _id)(count_over_time(probe_success{probe_url=~".*api.*"}[1h])) > 60
                            ), "_id_or_ns", "ocm-$1-$2", "namespace", "ocm-([^-]*)-([^-]*).*"),
                        "_id_or_ns", "$0", "_id", ".+")
                    unless on (_id_or_ns) (
                        label_replace(hypershift_cluster_alerts_disabled, "_id_or_ns", "$0", "_id", ".*")
                        or
                        label_replace(hypershift_cluster_alerts_disabled, "_id_or_ns", "$0", "exported_namespace", ".*")),
                    "_id_or_ns", "", "_id_or_ns", ".*")
              labels:
                long_window: 1h
                namespace: openshift-route-monitor-operator
                severity: critical
                short_window: 5m
                otel_collect: "true"
            - alert: api-ErrorBudgetBurn
              annotations:
                message: High error budget burn for openshift api
                runbook_url: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/api-ErrorBudgetBurn.md
              expr: |-
                label_replace(
                        label_replace(
                            label_replace((

                    1-(sum by (probe_url, mc_name, _mc_id, sector, region, _id)(sum_over_time(probe_success{probe_url=~".*api.*"}[30m]))/ sum by (probe_url, mc_name, _mc_id, sector, region, _id)(count_over_time(probe_success{probe_url=~".*api.*"}[30m])))> (6*(1-0.990)) and sum by (probe_url, mc_name, _mc_id, sector, region, _id)(count_over_time(probe_success{probe_url=~".*api.*"}[30m])) > 30
                    and
                    1-(sum by (probe_url, mc_name, _mc_id, sector, region, _id)(sum_over_time(probe_success{probe_url=~".*api.*"}[6h]))/ sum by (probe_url, mc_name, _mc_id, sector, region, _id)(count_over_time(probe_success{probe_url=~".*api.*"}[6h])))> (6*(1-0.990)) and sum by (probe_url, mc_name, _mc_id, sector, region, _id)(count_over_time(probe_success{probe_url=~".*api.*"}[6h])) > 360
                            ), "_id_or_ns", "ocm-$1-$2", "namespace", "ocm-([^-]*)-([^-]*).*"),
                        "_id_or_ns", "$0", "_id", ".+")
                    unless on (_id_or_ns) (
                        label_replace(hypershift_cluster_alerts_disabled, "_id_or_ns", "$0", "_id", ".*")
                        or
                        label_replace(hypershift_cluster_alerts_disabled, "_id_or_ns", "$0", "exported_namespace", ".*")),
                    "_id_or_ns", "", "_id_or_ns", ".*")
              labels:
                long_window: 6h
                namespace: openshift-route-monitor-operator
                severity: critical
                short_window: 30m
                otel_collect: "true"
            - alert: api-ErrorBudgetBurn
              annotations:
                message: High error budget burn for openshift api
                runbook_url: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/api-ErrorBudgetBurn.md
              expr: |-
                label_replace(
                    label_replace((

                1-(sum by (probe_url, region, sector, _mc_id, _id)(sum_over_time(probe_success{probe_url=~".*api.*"}[2h]))/ sum by (probe_url, region, sector, _mc_id, _id)(count_over_time(probe_success{probe_url=~".*api.*"}[2h])))> (3*(1-0.990)) and sum by (probe_url, region, sector, _mc_id, _id)(count_over_time(probe_success{probe_url=~".*api.*"}[2h])) > 120
                and
                1-(sum by (probe_url, region, sector, _mc_id, _id)(sum_over_time(probe_success{probe_url=~".*api.*"}[1d]))/ sum by (probe_url, region, sector, _mc_id, _id)(count_over_time(probe_success{probe_url=~".*api.*"}[1d])))> (3*(1-0.990)) and sum by (probe_url, region, sector, _mc_id, _id)(count_over_time(probe_success{probe_url=~".*api.*"}[1d])) > 1440
                        ), "_id_or_ns", "ocm-$1-$2", "namespace", "ocm-([^-]*)-([^-]*).*"),
                    "_id_or_ns", "$0", "_id", ".+")
                unless on (_id_or_ns) (
                    label_replace(hypershift_cluster_alerts_disabled, "_id_or_ns", "$0", "_id", ".*")
                    or
                    label_replace(hypershift_cluster_alerts_disabled, "_id_or_ns", "$0", "exported_namespace", ".*"))
              for: 1h
              labels:
                long_window: 1d
                namespace: openshift-route-monitor-operator
                severity: warning
                short_window: 2h
                otel_collect: "true"
            - alert: api-ErrorBudgetBurn
              annotations:
                message: High error budget burn for openshift api
                runbook_url: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/api-ErrorBudgetBurn.md
              expr: |-
                label_replace(
                    label_replace((

                1-(sum by (probe_url, region, sector, _mc_id, _id)(sum_over_time(probe_success{probe_url=~".*api.*"}[6h]))/ sum by (probe_url, region, sector, _mc_id, _id)(count_over_time(probe_success{probe_url=~".*api.*"}[6h])))> (1*(1-0.990)) and sum by (probe_url, region, sector, _mc_id, _id)(count_over_time(probe_success{probe_url=~".*api.*"}[6h])) > 360
                and
                1-(sum by (probe_url, region, sector, _mc_id, _id)(sum_over_time(probe_success{probe_url=~".*api.*"}[3d]))/ sum by (probe_url, region, sector, _mc_id, _id)(count_over_time(probe_success{probe_url=~".*api.*"}[3d])))> (1*(1-0.990)) and sum by (probe_url, region, sector, _mc_id, _id)(count_over_time(probe_success{probe_url=~".*api.*"}[3d])) > 4320
                        ), "_id_or_ns", "ocm-$1-$2", "namespace", "ocm-([^-]*)-([^-]*).*"),
                    "_id_or_ns", "$0", "_id", ".+")
                unless on (_id_or_ns) (
                    label_replace(hypershift_cluster_alerts_disabled, "_id_or_ns", "$0", "_id", ".*")
                    or
                    label_replace(hypershift_cluster_alerts_disabled, "_id_or_ns", "$0", "exported_namespace", ".*"))
              for: 3h
              labels:
                long_window: 3d
                namespace: openshift-route-monitor-operator
                severity: warning
                short_window: 6h
                otel_collect: "true"
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: sre-kube-apiserver-rules
      namespace: ${NAMESPACE}
      labels:
        operator.thanos.io/prometheus-rule: "true"
        operator.thanos.io/tenant: ${TENANT}
    spec:
      groups:
        - name: osd-kube-apiserver-rules
          interval: 1m
          rules:
            - alert: KubeAPIServerDown
              annotations:
                description: The KubeAPIServer pods in the HCP Namespace are not ready, blocking all cluster operations.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/hypershift/alerts/KubeAPIServerDown.md
                summary: No ready KubeAPIServer pods in the HCP namespace.
              labels:
                severity: critical
                otel_collect: "true"
              expr: sre:kube_apiserver:pod_status_ready == 0
              for: 15m
            - alert: KubeAPIServerDegraded
              annotations:
                description: A KubeAPIServer deployment in the HCP namespace has unavailable replicas, risking reduced capacity or latency.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/hypershift/alerts/KubeAPIServerDegraded.md
                summary: Unavailable replicas in KubeAPIServer deployment in the HCP namespace.
              expr: |
                sre:kube_apiserver:deployment_pods_unavailable > 0
              for: 15m
              labels:
                severity: warning
                otel_collect: "true"
            - alert: OpenshiftAPIServerDown
              annotations:
                description: The OpenShiftAPIServer pods in the HCP Namespace are not ready, blocking openshift-specific operations to be blocked.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/hypershift/alerts/OpenshiftAPIServerDown.md
                summary: No ready OpenShiftAPIServer pods in the HCP namespace.
              labels:
                severity: critical
                otel_collect: "true"
              expr: sre:openshift_apiserver:pod_status_ready == 0
              for: 15m
            - alert: OpenshiftAPIServerDegraded
              annotations:
                description: An OpenShiftAPIServer deployment in the HCP namespace has unavailable replicas, risking reduced capacity or latency.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/hypershift/alerts/OpenshiftAPIServerDegraded.md
                summary: Unavailable replicas in OpenShiftAPIServer deployment in the HCP namespace.
              expr: |
                sre:openshift_apiserver:deployment_pods_unavailable > 0
              for: 15m
              labels:
                severity: warning
                otel_collect: "true"
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: sae-deployment
      namespace: ${NAMESPACE}
      labels:
        operator.thanos.io/prometheus-rule: "true"
        operator.thanos.io/tenant: ${TENANT}
    spec:
      groups:
        - name: SAEDeploymentErrors
          interval: 60s
          rules:
            - alert: SAEDeploymentMissing
              annotations:
                description: The SAE deployment is missing for {{ $labels.namespace }}. Splunk audit log forwarding is not functional.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/hypershift/alerts/SAEDeploymentMissing.md
                summary: SAE deployment missing
              expr: |
                (sum by (_id, _mc_id, namespace, region, sector) (kube_deployment_labels{namespace=~"ocm-.*-.*", deployment="splunk-audit-exporter"}))
                unless on (_id, _mc_id, namespace)
                (sum by (_id, _mc_id, namespace, region, sector) (kube_deployment_status_replicas_available{namespace=~"ocm-.*-.*", deployment="splunk-audit-exporter"} > 0))
              for: 15m
              labels:
                severity: warning
                otel_collect: "true"
            - alert: SAEDeploymentDown
              annotations:
                description: The SAE deployment for {{ $labels.namespace }} has 0 available replicas. Splunk audit log forwarding is not functional.
                runbook_url: https://github.com/openshift/ops-sop/blob/master/hypershift/alerts/SAEDeploymentDown.md
                summary: SAE deployment down
              expr: kube_deployment_status_replicas_available{namespace=~"ocm-.*-.*", deployment="splunk-audit-exporter"} == 0
              for: 15m
              labels:
                severity: critical
                otel_collect: "true"
