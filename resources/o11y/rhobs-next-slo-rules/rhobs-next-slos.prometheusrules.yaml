---
$schema: /openshift/prometheus-rule-1.yml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: app-sre
    role: alert-rules
  name: rhobs-next-slos
spec:
  groups:
  - interval: 2m30s
    name: api-metrics-write-availability-slo-increase
    rules:
    - expr: sum by (code) (increase(http_requests_total{group="metricsv1",handler="receive",job="observatorium-api"}[4w]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-write-availability-slo
      record: http_requests:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /receive handler is burning too much error budget to guarantee
          availability SLOs.
        message: API /receive handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteAvailabilityErrorBudgetBurning
      expr: absent(http_requests_total{group="metricsv1",handler="receive",job="observatorium-api"})
        == 1
      for: 2m
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-api
        service: rhobs.regional
        severity: medium
        slo: api-metrics-write-availability-slo
  - interval: 30s
    name: api-metrics-write-availability-slo
    rules:
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="receive",job="observatorium-api"}[5m]))
        / sum(rate(http_requests_total{group="metricsv1",handler="receive",job="observatorium-api"}[5m]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-write-availability-slo
      record: http_requests:burnrate5m
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="receive",job="observatorium-api"}[30m]))
        / sum(rate(http_requests_total{group="metricsv1",handler="receive",job="observatorium-api"}[30m]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-write-availability-slo
      record: http_requests:burnrate30m
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="receive",job="observatorium-api"}[1h]))
        / sum(rate(http_requests_total{group="metricsv1",handler="receive",job="observatorium-api"}[1h]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-write-availability-slo
      record: http_requests:burnrate1h
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="receive",job="observatorium-api"}[2h]))
        / sum(rate(http_requests_total{group="metricsv1",handler="receive",job="observatorium-api"}[2h]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-write-availability-slo
      record: http_requests:burnrate2h
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="receive",job="observatorium-api"}[6h]))
        / sum(rate(http_requests_total{group="metricsv1",handler="receive",job="observatorium-api"}[6h]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-write-availability-slo
      record: http_requests:burnrate6h
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="receive",job="observatorium-api"}[1d]))
        / sum(rate(http_requests_total{group="metricsv1",handler="receive",job="observatorium-api"}[1d]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-write-availability-slo
      record: http_requests:burnrate1d
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="receive",job="observatorium-api"}[4d]))
        / sum(rate(http_requests_total{group="metricsv1",handler="receive",job="observatorium-api"}[4d]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-write-availability-slo
      record: http_requests:burnrate4d
    - alert: APIMetricsWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /receive handler is burning too much error budget to guarantee
          availability SLOs.
        message: API /receive handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate5m{group="metricsv1",handler="receive",job="observatorium-api",slo="api-metrics-write-availability-slo"}
        > (14 * (1-0.9990000000000001)) and http_requests:burnrate1h{group="metricsv1",handler="receive",job="observatorium-api",slo="api-metrics-write-availability-slo"}
        > (14 * (1-0.9990000000000001))
      for: 2m0s
      labels:
        exhaustion: 2d
        group: metricsv1
        handler: receive
        job: observatorium-api
        long_burnrate_window: 1h
        service: rhobs.regional
        severity: critical
        short_burnrate_window: 5m
        slo: api-metrics-write-availability-slo
    - alert: APIMetricsWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /receive handler is burning too much error budget to guarantee
          availability SLOs.
        message: API /receive handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate30m{group="metricsv1",handler="receive",job="observatorium-api",slo="api-metrics-write-availability-slo"}
        > (7 * (1-0.9990000000000001)) and http_requests:burnrate6h{group="metricsv1",handler="receive",job="observatorium-api",slo="api-metrics-write-availability-slo"}
        > (7 * (1-0.9990000000000001))
      for: 15m0s
      labels:
        exhaustion: 4d
        group: metricsv1
        handler: receive
        job: observatorium-api
        long_burnrate_window: 6h
        service: rhobs.regional
        severity: critical
        short_burnrate_window: 30m
        slo: api-metrics-write-availability-slo
    - alert: APIMetricsWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /receive handler is burning too much error budget to guarantee
          availability SLOs.
        message: API /receive handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate2h{group="metricsv1",handler="receive",job="observatorium-api",slo="api-metrics-write-availability-slo"}
        > (2 * (1-0.9990000000000001)) and http_requests:burnrate1d{group="metricsv1",handler="receive",job="observatorium-api",slo="api-metrics-write-availability-slo"}
        > (2 * (1-0.9990000000000001))
      for: 1h0m0s
      labels:
        exhaustion: 2w
        group: metricsv1
        handler: receive
        job: observatorium-api
        long_burnrate_window: 1d
        service: rhobs.regional
        severity: warning
        short_burnrate_window: 2h
        slo: api-metrics-write-availability-slo
    - alert: APIMetricsWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /receive handler is burning too much error budget to guarantee
          availability SLOs.
        message: API /receive handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate6h{group="metricsv1",handler="receive",job="observatorium-api",slo="api-metrics-write-availability-slo"}
        > (1 * (1-0.9990000000000001)) and http_requests:burnrate4d{group="metricsv1",handler="receive",job="observatorium-api",slo="api-metrics-write-availability-slo"}
        > (1 * (1-0.9990000000000001))
      for: 3h0m0s
      labels:
        exhaustion: 4w
        group: metricsv1
        handler: receive
        job: observatorium-api
        long_burnrate_window: 4d
        service: rhobs.regional
        severity: warning
        short_burnrate_window: 6h
        slo: api-metrics-write-availability-slo
  - interval: 30s
    name: api-metrics-write-availability-slo-generic
    rules:
    - expr: "0.9990000000000001"
      labels:
        service: rhobs.regional
        slo: api-metrics-write-availability-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        service: rhobs.regional
        slo: api-metrics-write-availability-slo
      record: pyrra_window
    - expr: 1 - sum(http_requests:increase4w{code=~"^5..$",group="metricsv1",handler="receive",job="observatorium-api",slo="api-metrics-write-availability-slo"}
        or vector(0)) / sum(http_requests:increase4w{group="metricsv1",handler="receive",job="observatorium-api",slo="api-metrics-write-availability-slo"})
      labels:
        service: rhobs.regional
        slo: api-metrics-write-availability-slo
      record: pyrra_availability
    - expr: sum(http_requests_total{group="metricsv1",handler="receive",job="observatorium-api"})
      labels:
        service: rhobs.regional
        slo: api-metrics-write-availability-slo
      record: pyrra_requests_total
    - expr: sum(http_requests_total{code=~"^5..$",group="metricsv1",handler="receive",job="observatorium-api"}
        or vector(0))
      labels:
        service: rhobs.regional
        slo: api-metrics-write-availability-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: api-metrics-query-availability-slo-increase
    rules:
    - expr: sum by (code) (increase(http_requests_total{group="metricsv1",handler="query",job="observatorium-api"}[4w]))
      labels:
        group: metricsv1
        handler: query
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-query-availability-slo
      record: http_requests:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /query handler is burning too much error budget to guarantee
          availability SLOs.
        message: API /query handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsQueryAvailabilityErrorBudgetBurning
      expr: absent(http_requests_total{group="metricsv1",handler="query",job="observatorium-api"})
        == 1
      for: 2m
      labels:
        group: metricsv1
        handler: query
        job: observatorium-api
        service: rhobs.regional
        severity: medium
        slo: api-metrics-query-availability-slo
  - interval: 30s
    name: api-metrics-query-availability-slo
    rules:
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query",job="observatorium-api"}[5m]))
        / sum(rate(http_requests_total{group="metricsv1",handler="query",job="observatorium-api"}[5m]))
      labels:
        group: metricsv1
        handler: query
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-query-availability-slo
      record: http_requests:burnrate5m
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query",job="observatorium-api"}[30m]))
        / sum(rate(http_requests_total{group="metricsv1",handler="query",job="observatorium-api"}[30m]))
      labels:
        group: metricsv1
        handler: query
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-query-availability-slo
      record: http_requests:burnrate30m
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query",job="observatorium-api"}[1h]))
        / sum(rate(http_requests_total{group="metricsv1",handler="query",job="observatorium-api"}[1h]))
      labels:
        group: metricsv1
        handler: query
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-query-availability-slo
      record: http_requests:burnrate1h
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query",job="observatorium-api"}[2h]))
        / sum(rate(http_requests_total{group="metricsv1",handler="query",job="observatorium-api"}[2h]))
      labels:
        group: metricsv1
        handler: query
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-query-availability-slo
      record: http_requests:burnrate2h
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query",job="observatorium-api"}[6h]))
        / sum(rate(http_requests_total{group="metricsv1",handler="query",job="observatorium-api"}[6h]))
      labels:
        group: metricsv1
        handler: query
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-query-availability-slo
      record: http_requests:burnrate6h
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query",job="observatorium-api"}[1d]))
        / sum(rate(http_requests_total{group="metricsv1",handler="query",job="observatorium-api"}[1d]))
      labels:
        group: metricsv1
        handler: query
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-query-availability-slo
      record: http_requests:burnrate1d
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query",job="observatorium-api"}[4d]))
        / sum(rate(http_requests_total{group="metricsv1",handler="query",job="observatorium-api"}[4d]))
      labels:
        group: metricsv1
        handler: query
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-query-availability-slo
      record: http_requests:burnrate4d
    - alert: APIMetricsQueryAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /query handler is burning too much error budget to guarantee
          availability SLOs.
        message: API /query handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsQueryAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate5m{group="metricsv1",handler="query",job="observatorium-api",slo="api-metrics-query-availability-slo"}
        > (14 * (1-0.9990000000000001)) and http_requests:burnrate1h{group="metricsv1",handler="query",job="observatorium-api",slo="api-metrics-query-availability-slo"}
        > (14 * (1-0.9990000000000001))
      for: 2m0s
      labels:
        exhaustion: 2d
        group: metricsv1
        handler: query
        job: observatorium-api
        long_burnrate_window: 1h
        service: rhobs.regional
        severity: critical
        short_burnrate_window: 5m
        slo: api-metrics-query-availability-slo
    - alert: APIMetricsQueryAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /query handler is burning too much error budget to guarantee
          availability SLOs.
        message: API /query handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsQueryAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate30m{group="metricsv1",handler="query",job="observatorium-api",slo="api-metrics-query-availability-slo"}
        > (7 * (1-0.9990000000000001)) and http_requests:burnrate6h{group="metricsv1",handler="query",job="observatorium-api",slo="api-metrics-query-availability-slo"}
        > (7 * (1-0.9990000000000001))
      for: 15m0s
      labels:
        exhaustion: 4d
        group: metricsv1
        handler: query
        job: observatorium-api
        long_burnrate_window: 6h
        service: rhobs.regional
        severity: critical
        short_burnrate_window: 30m
        slo: api-metrics-query-availability-slo
    - alert: APIMetricsQueryAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /query handler is burning too much error budget to guarantee
          availability SLOs.
        message: API /query handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsQueryAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate2h{group="metricsv1",handler="query",job="observatorium-api",slo="api-metrics-query-availability-slo"}
        > (2 * (1-0.9990000000000001)) and http_requests:burnrate1d{group="metricsv1",handler="query",job="observatorium-api",slo="api-metrics-query-availability-slo"}
        > (2 * (1-0.9990000000000001))
      for: 1h0m0s
      labels:
        exhaustion: 2w
        group: metricsv1
        handler: query
        job: observatorium-api
        long_burnrate_window: 1d
        service: rhobs.regional
        severity: warning
        short_burnrate_window: 2h
        slo: api-metrics-query-availability-slo
    - alert: APIMetricsQueryAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /query handler is burning too much error budget to guarantee
          availability SLOs.
        message: API /query handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsQueryAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate6h{group="metricsv1",handler="query",job="observatorium-api",slo="api-metrics-query-availability-slo"}
        > (1 * (1-0.9990000000000001)) and http_requests:burnrate4d{group="metricsv1",handler="query",job="observatorium-api",slo="api-metrics-query-availability-slo"}
        > (1 * (1-0.9990000000000001))
      for: 3h0m0s
      labels:
        exhaustion: 4w
        group: metricsv1
        handler: query
        job: observatorium-api
        long_burnrate_window: 4d
        service: rhobs.regional
        severity: warning
        short_burnrate_window: 6h
        slo: api-metrics-query-availability-slo
  - interval: 30s
    name: api-metrics-query-availability-slo-generic
    rules:
    - expr: "0.9990000000000001"
      labels:
        service: rhobs.regional
        slo: api-metrics-query-availability-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        service: rhobs.regional
        slo: api-metrics-query-availability-slo
      record: pyrra_window
    - expr: 1 - sum(http_requests:increase4w{code=~"^5..$",group="metricsv1",handler="query",job="observatorium-api",slo="api-metrics-query-availability-slo"}
        or vector(0)) / sum(http_requests:increase4w{group="metricsv1",handler="query",job="observatorium-api",slo="api-metrics-query-availability-slo"})
      labels:
        service: rhobs.regional
        slo: api-metrics-query-availability-slo
      record: pyrra_availability
    - expr: sum(http_requests_total{group="metricsv1",handler="query",job="observatorium-api"})
      labels:
        service: rhobs.regional
        slo: api-metrics-query-availability-slo
      record: pyrra_requests_total
    - expr: sum(http_requests_total{code=~"^5..$",group="metricsv1",handler="query",job="observatorium-api"}
        or vector(0))
      labels:
        service: rhobs.regional
        slo: api-metrics-query-availability-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: api-metrics-query-range-availability-slo-increase
    rules:
    - expr: sum by (code) (increase(http_requests_total{group="metricsv1",handler="query_range",job="observatorium-api"}[4w]))
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-query-range-availability-slo
      record: http_requests:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /query_range handler is burning too much error budget to
          guarantee availability SLOs.
        message: API /query_range handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsQueryRangeAvailabilityErrorBudgetBurning
      expr: absent(http_requests_total{group="metricsv1",handler="query_range",job="observatorium-api"})
        == 1
      for: 2m
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-api
        service: rhobs.regional
        severity: medium
        slo: api-metrics-query-range-availability-slo
  - interval: 30s
    name: api-metrics-query-range-availability-slo
    rules:
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query_range",job="observatorium-api"}[5m]))
        / sum(rate(http_requests_total{group="metricsv1",handler="query_range",job="observatorium-api"}[5m]))
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-query-range-availability-slo
      record: http_requests:burnrate5m
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query_range",job="observatorium-api"}[30m]))
        / sum(rate(http_requests_total{group="metricsv1",handler="query_range",job="observatorium-api"}[30m]))
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-query-range-availability-slo
      record: http_requests:burnrate30m
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query_range",job="observatorium-api"}[1h]))
        / sum(rate(http_requests_total{group="metricsv1",handler="query_range",job="observatorium-api"}[1h]))
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-query-range-availability-slo
      record: http_requests:burnrate1h
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query_range",job="observatorium-api"}[2h]))
        / sum(rate(http_requests_total{group="metricsv1",handler="query_range",job="observatorium-api"}[2h]))
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-query-range-availability-slo
      record: http_requests:burnrate2h
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query_range",job="observatorium-api"}[6h]))
        / sum(rate(http_requests_total{group="metricsv1",handler="query_range",job="observatorium-api"}[6h]))
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-query-range-availability-slo
      record: http_requests:burnrate6h
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query_range",job="observatorium-api"}[1d]))
        / sum(rate(http_requests_total{group="metricsv1",handler="query_range",job="observatorium-api"}[1d]))
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-query-range-availability-slo
      record: http_requests:burnrate1d
    - expr: sum(rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query_range",job="observatorium-api"}[4d]))
        / sum(rate(http_requests_total{group="metricsv1",handler="query_range",job="observatorium-api"}[4d]))
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-query-range-availability-slo
      record: http_requests:burnrate4d
    - alert: APIMetricsQueryRangeAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /query_range handler is burning too much error budget to
          guarantee availability SLOs.
        message: API /query_range handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsQueryRangeAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate5m{group="metricsv1",handler="query_range",job="observatorium-api",slo="api-metrics-query-range-availability-slo"}
        > (14 * (1-0.9990000000000001)) and http_requests:burnrate1h{group="metricsv1",handler="query_range",job="observatorium-api",slo="api-metrics-query-range-availability-slo"}
        > (14 * (1-0.9990000000000001))
      for: 2m0s
      labels:
        exhaustion: 2d
        group: metricsv1
        handler: query_range
        job: observatorium-api
        long_burnrate_window: 1h
        service: rhobs.regional
        severity: critical
        short_burnrate_window: 5m
        slo: api-metrics-query-range-availability-slo
    - alert: APIMetricsQueryRangeAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /query_range handler is burning too much error budget to
          guarantee availability SLOs.
        message: API /query_range handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsQueryRangeAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate30m{group="metricsv1",handler="query_range",job="observatorium-api",slo="api-metrics-query-range-availability-slo"}
        > (7 * (1-0.9990000000000001)) and http_requests:burnrate6h{group="metricsv1",handler="query_range",job="observatorium-api",slo="api-metrics-query-range-availability-slo"}
        > (7 * (1-0.9990000000000001))
      for: 15m0s
      labels:
        exhaustion: 4d
        group: metricsv1
        handler: query_range
        job: observatorium-api
        long_burnrate_window: 6h
        service: rhobs.regional
        severity: critical
        short_burnrate_window: 30m
        slo: api-metrics-query-range-availability-slo
    - alert: APIMetricsQueryRangeAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /query_range handler is burning too much error budget to
          guarantee availability SLOs.
        message: API /query_range handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsQueryRangeAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate2h{group="metricsv1",handler="query_range",job="observatorium-api",slo="api-metrics-query-range-availability-slo"}
        > (2 * (1-0.9990000000000001)) and http_requests:burnrate1d{group="metricsv1",handler="query_range",job="observatorium-api",slo="api-metrics-query-range-availability-slo"}
        > (2 * (1-0.9990000000000001))
      for: 1h0m0s
      labels:
        exhaustion: 2w
        group: metricsv1
        handler: query_range
        job: observatorium-api
        long_burnrate_window: 1d
        service: rhobs.regional
        severity: warning
        short_burnrate_window: 2h
        slo: api-metrics-query-range-availability-slo
    - alert: APIMetricsQueryRangeAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /query_range handler is burning too much error budget to
          guarantee availability SLOs.
        message: API /query_range handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsQueryRangeAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate6h{group="metricsv1",handler="query_range",job="observatorium-api",slo="api-metrics-query-range-availability-slo"}
        > (1 * (1-0.9990000000000001)) and http_requests:burnrate4d{group="metricsv1",handler="query_range",job="observatorium-api",slo="api-metrics-query-range-availability-slo"}
        > (1 * (1-0.9990000000000001))
      for: 3h0m0s
      labels:
        exhaustion: 4w
        group: metricsv1
        handler: query_range
        job: observatorium-api
        long_burnrate_window: 4d
        service: rhobs.regional
        severity: warning
        short_burnrate_window: 6h
        slo: api-metrics-query-range-availability-slo
  - interval: 30s
    name: api-metrics-query-range-availability-slo-generic
    rules:
    - expr: "0.9990000000000001"
      labels:
        service: rhobs.regional
        slo: api-metrics-query-range-availability-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        service: rhobs.regional
        slo: api-metrics-query-range-availability-slo
      record: pyrra_window
    - expr: 1 - sum(http_requests:increase4w{code=~"^5..$",group="metricsv1",handler="query_range",job="observatorium-api",slo="api-metrics-query-range-availability-slo"}
        or vector(0)) / sum(http_requests:increase4w{group="metricsv1",handler="query_range",job="observatorium-api",slo="api-metrics-query-range-availability-slo"})
      labels:
        service: rhobs.regional
        slo: api-metrics-query-range-availability-slo
      record: pyrra_availability
    - expr: sum(http_requests_total{group="metricsv1",handler="query_range",job="observatorium-api"})
      labels:
        service: rhobs.regional
        slo: api-metrics-query-range-availability-slo
      record: pyrra_requests_total
    - expr: sum(http_requests_total{code=~"^5..$",group="metricsv1",handler="query_range",job="observatorium-api"}
        or vector(0))
      labels:
        service: rhobs.regional
        slo: api-metrics-query-range-availability-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: api-alerting-availability-slo-increase
    rules:
    - expr: sum by (code) (increase(thanos_alert_sender_alerts_dropped_total{container="thanos-ruler"}[4w]))
      labels:
        container: thanos-ruler
        service: rhobs.regional
        slo: api-alerting-availability-slo
      record: thanos_alert_sender_alerts_dropped:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API Thanos Rule failing to send alerts to Alertmanager and is
          burning too much error budget to guarantee availability SLOs.
        message: API Thanos Rule is burning too much error budget to guarantee availability
          SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIAlertmanagerAvailabilityErrorBudgetBurning
      expr: absent(thanos_alert_sender_alerts_dropped_total{container="thanos-ruler"})
        == 1
      for: 2m
      labels:
        service: rhobs.regional
        severity: medium
        slo: api-alerting-availability-slo
  - interval: 30s
    name: api-alerting-availability-slo
    rules:
    - expr: sum(rate(thanos_alert_sender_alerts_dropped_total{code=~"^5..$",container="thanos-ruler"}[5m]))
        / sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-ruler"}[5m]))
      labels:
        container: thanos-ruler
        service: rhobs.regional
        slo: api-alerting-availability-slo
      record: thanos_alert_sender_alerts_dropped:burnrate5m
    - expr: sum(rate(thanos_alert_sender_alerts_dropped_total{code=~"^5..$",container="thanos-ruler"}[30m]))
        / sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-ruler"}[30m]))
      labels:
        container: thanos-ruler
        service: rhobs.regional
        slo: api-alerting-availability-slo
      record: thanos_alert_sender_alerts_dropped:burnrate30m
    - expr: sum(rate(thanos_alert_sender_alerts_dropped_total{code=~"^5..$",container="thanos-ruler"}[1h]))
        / sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-ruler"}[1h]))
      labels:
        container: thanos-ruler
        service: rhobs.regional
        slo: api-alerting-availability-slo
      record: thanos_alert_sender_alerts_dropped:burnrate1h
    - expr: sum(rate(thanos_alert_sender_alerts_dropped_total{code=~"^5..$",container="thanos-ruler"}[2h]))
        / sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-ruler"}[2h]))
      labels:
        container: thanos-ruler
        service: rhobs.regional
        slo: api-alerting-availability-slo
      record: thanos_alert_sender_alerts_dropped:burnrate2h
    - expr: sum(rate(thanos_alert_sender_alerts_dropped_total{code=~"^5..$",container="thanos-ruler"}[6h]))
        / sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-ruler"}[6h]))
      labels:
        container: thanos-ruler
        service: rhobs.regional
        slo: api-alerting-availability-slo
      record: thanos_alert_sender_alerts_dropped:burnrate6h
    - expr: sum(rate(thanos_alert_sender_alerts_dropped_total{code=~"^5..$",container="thanos-ruler"}[1d]))
        / sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-ruler"}[1d]))
      labels:
        container: thanos-ruler
        service: rhobs.regional
        slo: api-alerting-availability-slo
      record: thanos_alert_sender_alerts_dropped:burnrate1d
    - expr: sum(rate(thanos_alert_sender_alerts_dropped_total{code=~"^5..$",container="thanos-ruler"}[4d]))
        / sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-ruler"}[4d]))
      labels:
        container: thanos-ruler
        service: rhobs.regional
        slo: api-alerting-availability-slo
      record: thanos_alert_sender_alerts_dropped:burnrate4d
    - alert: APIAlertmanagerAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API Thanos Rule failing to send alerts to Alertmanager and is
          burning too much error budget to guarantee availability SLOs.
        message: API Thanos Rule is burning too much error budget to guarantee availability
          SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIAlertmanagerAvailabilityErrorBudgetBurning
      expr: thanos_alert_sender_alerts_dropped:burnrate5m{container="thanos-ruler",slo="api-alerting-availability-slo"}
        > (14 * (1-0.9990000000000001)) and thanos_alert_sender_alerts_dropped:burnrate1h{container="thanos-ruler",slo="api-alerting-availability-slo"}
        > (14 * (1-0.9990000000000001))
      for: 2m0s
      labels:
        exhaustion: 2d
        long_burnrate_window: 1h
        service: rhobs.regional
        severity: critical
        short_burnrate_window: 5m
        slo: api-alerting-availability-slo
    - alert: APIAlertmanagerAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API Thanos Rule failing to send alerts to Alertmanager and is
          burning too much error budget to guarantee availability SLOs.
        message: API Thanos Rule is burning too much error budget to guarantee availability
          SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIAlertmanagerAvailabilityErrorBudgetBurning
      expr: thanos_alert_sender_alerts_dropped:burnrate30m{container="thanos-ruler",slo="api-alerting-availability-slo"}
        > (7 * (1-0.9990000000000001)) and thanos_alert_sender_alerts_dropped:burnrate6h{container="thanos-ruler",slo="api-alerting-availability-slo"}
        > (7 * (1-0.9990000000000001))
      for: 15m0s
      labels:
        exhaustion: 4d
        long_burnrate_window: 6h
        service: rhobs.regional
        severity: critical
        short_burnrate_window: 30m
        slo: api-alerting-availability-slo
    - alert: APIAlertmanagerAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API Thanos Rule failing to send alerts to Alertmanager and is
          burning too much error budget to guarantee availability SLOs.
        message: API Thanos Rule is burning too much error budget to guarantee availability
          SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIAlertmanagerAvailabilityErrorBudgetBurning
      expr: thanos_alert_sender_alerts_dropped:burnrate2h{container="thanos-ruler",slo="api-alerting-availability-slo"}
        > (2 * (1-0.9990000000000001)) and thanos_alert_sender_alerts_dropped:burnrate1d{container="thanos-ruler",slo="api-alerting-availability-slo"}
        > (2 * (1-0.9990000000000001))
      for: 1h0m0s
      labels:
        exhaustion: 2w
        long_burnrate_window: 1d
        service: rhobs.regional
        severity: warning
        short_burnrate_window: 2h
        slo: api-alerting-availability-slo
    - alert: APIAlertmanagerAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API Thanos Rule failing to send alerts to Alertmanager and is
          burning too much error budget to guarantee availability SLOs.
        message: API Thanos Rule is burning too much error budget to guarantee availability
          SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIAlertmanagerAvailabilityErrorBudgetBurning
      expr: thanos_alert_sender_alerts_dropped:burnrate6h{container="thanos-ruler",slo="api-alerting-availability-slo"}
        > (1 * (1-0.9990000000000001)) and thanos_alert_sender_alerts_dropped:burnrate4d{container="thanos-ruler",slo="api-alerting-availability-slo"}
        > (1 * (1-0.9990000000000001))
      for: 3h0m0s
      labels:
        exhaustion: 4w
        long_burnrate_window: 4d
        service: rhobs.regional
        severity: warning
        short_burnrate_window: 6h
        slo: api-alerting-availability-slo
  - interval: 30s
    name: api-alerting-availability-slo-generic
    rules:
    - expr: "0.9990000000000001"
      labels:
        service: rhobs.regional
        slo: api-alerting-availability-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        service: rhobs.regional
        slo: api-alerting-availability-slo
      record: pyrra_window
    - expr: 1 - sum(thanos_alert_sender_alerts_dropped:increase4w{code=~"^5..$",container="thanos-ruler",slo="api-alerting-availability-slo"}
        or vector(0)) / sum(thanos_alert_sender_alerts_dropped:increase4w{container="thanos-ruler",slo="api-alerting-availability-slo"})
      labels:
        service: rhobs.regional
        slo: api-alerting-availability-slo
      record: pyrra_availability
    - expr: sum(thanos_alert_sender_alerts_dropped_total{container="thanos-ruler"})
      labels:
        service: rhobs.regional
        slo: api-alerting-availability-slo
      record: pyrra_requests_total
    - expr: sum(thanos_alert_sender_alerts_dropped_total{code=~"^5..$",container="thanos-ruler"}
        or vector(0))
      labels:
        service: rhobs.regional
        slo: api-alerting-availability-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: api-alerting-notif-availability-slo-increase
    rules:
    - expr: sum by (code) (increase(alertmanager_notifications_failed_total{job="alertmanager"}[4w]))
      labels:
        job: alertmanager
        service: rhobs.regional
        slo: api-alerting-notif-availability-slo
      record: alertmanager_notifications_failed:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API Alertmanager failing to deliver alerts to upstream targets
          and is burning too much error budget to guarantee availability SLOs.
        message: API Alertmanager is burning too much error budget to guarantee availability
          SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning
      expr: absent(alertmanager_notifications_failed_total{job="alertmanager"}) ==
        1
      for: 2m
      labels:
        job: alertmanager
        service: rhobs.regional
        severity: medium
        slo: api-alerting-notif-availability-slo
  - interval: 30s
    name: api-alerting-notif-availability-slo
    rules:
    - expr: sum(rate(alertmanager_notifications_failed_total{code=~"^5..$",job="alertmanager"}[5m]))
        / sum(rate(alertmanager_notifications_failed_total{job="alertmanager"}[5m]))
      labels:
        job: alertmanager
        service: rhobs.regional
        slo: api-alerting-notif-availability-slo
      record: alertmanager_notifications_failed:burnrate5m
    - expr: sum(rate(alertmanager_notifications_failed_total{code=~"^5..$",job="alertmanager"}[30m]))
        / sum(rate(alertmanager_notifications_failed_total{job="alertmanager"}[30m]))
      labels:
        job: alertmanager
        service: rhobs.regional
        slo: api-alerting-notif-availability-slo
      record: alertmanager_notifications_failed:burnrate30m
    - expr: sum(rate(alertmanager_notifications_failed_total{code=~"^5..$",job="alertmanager"}[1h]))
        / sum(rate(alertmanager_notifications_failed_total{job="alertmanager"}[1h]))
      labels:
        job: alertmanager
        service: rhobs.regional
        slo: api-alerting-notif-availability-slo
      record: alertmanager_notifications_failed:burnrate1h
    - expr: sum(rate(alertmanager_notifications_failed_total{code=~"^5..$",job="alertmanager"}[2h]))
        / sum(rate(alertmanager_notifications_failed_total{job="alertmanager"}[2h]))
      labels:
        job: alertmanager
        service: rhobs.regional
        slo: api-alerting-notif-availability-slo
      record: alertmanager_notifications_failed:burnrate2h
    - expr: sum(rate(alertmanager_notifications_failed_total{code=~"^5..$",job="alertmanager"}[6h]))
        / sum(rate(alertmanager_notifications_failed_total{job="alertmanager"}[6h]))
      labels:
        job: alertmanager
        service: rhobs.regional
        slo: api-alerting-notif-availability-slo
      record: alertmanager_notifications_failed:burnrate6h
    - expr: sum(rate(alertmanager_notifications_failed_total{code=~"^5..$",job="alertmanager"}[1d]))
        / sum(rate(alertmanager_notifications_failed_total{job="alertmanager"}[1d]))
      labels:
        job: alertmanager
        service: rhobs.regional
        slo: api-alerting-notif-availability-slo
      record: alertmanager_notifications_failed:burnrate1d
    - expr: sum(rate(alertmanager_notifications_failed_total{code=~"^5..$",job="alertmanager"}[4d]))
        / sum(rate(alertmanager_notifications_failed_total{job="alertmanager"}[4d]))
      labels:
        job: alertmanager
        service: rhobs.regional
        slo: api-alerting-notif-availability-slo
      record: alertmanager_notifications_failed:burnrate4d
    - alert: APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API Alertmanager failing to deliver alerts to upstream targets
          and is burning too much error budget to guarantee availability SLOs.
        message: API Alertmanager is burning too much error budget to guarantee availability
          SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning
      expr: alertmanager_notifications_failed:burnrate5m{job="alertmanager",slo="api-alerting-notif-availability-slo"}
        > (14 * (1-0.9990000000000001)) and alertmanager_notifications_failed:burnrate1h{job="alertmanager",slo="api-alerting-notif-availability-slo"}
        > (14 * (1-0.9990000000000001))
      for: 2m0s
      labels:
        exhaustion: 2d
        job: alertmanager
        long_burnrate_window: 1h
        service: rhobs.regional
        severity: critical
        short_burnrate_window: 5m
        slo: api-alerting-notif-availability-slo
    - alert: APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API Alertmanager failing to deliver alerts to upstream targets
          and is burning too much error budget to guarantee availability SLOs.
        message: API Alertmanager is burning too much error budget to guarantee availability
          SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning
      expr: alertmanager_notifications_failed:burnrate30m{job="alertmanager",slo="api-alerting-notif-availability-slo"}
        > (7 * (1-0.9990000000000001)) and alertmanager_notifications_failed:burnrate6h{job="alertmanager",slo="api-alerting-notif-availability-slo"}
        > (7 * (1-0.9990000000000001))
      for: 15m0s
      labels:
        exhaustion: 4d
        job: alertmanager
        long_burnrate_window: 6h
        service: rhobs.regional
        severity: critical
        short_burnrate_window: 30m
        slo: api-alerting-notif-availability-slo
    - alert: APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API Alertmanager failing to deliver alerts to upstream targets
          and is burning too much error budget to guarantee availability SLOs.
        message: API Alertmanager is burning too much error budget to guarantee availability
          SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning
      expr: alertmanager_notifications_failed:burnrate2h{job="alertmanager",slo="api-alerting-notif-availability-slo"}
        > (2 * (1-0.9990000000000001)) and alertmanager_notifications_failed:burnrate1d{job="alertmanager",slo="api-alerting-notif-availability-slo"}
        > (2 * (1-0.9990000000000001))
      for: 1h0m0s
      labels:
        exhaustion: 2w
        job: alertmanager
        long_burnrate_window: 1d
        service: rhobs.regional
        severity: warning
        short_burnrate_window: 2h
        slo: api-alerting-notif-availability-slo
    - alert: APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API Alertmanager failing to deliver alerts to upstream targets
          and is burning too much error budget to guarantee availability SLOs.
        message: API Alertmanager is burning too much error budget to guarantee availability
          SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning
      expr: alertmanager_notifications_failed:burnrate6h{job="alertmanager",slo="api-alerting-notif-availability-slo"}
        > (1 * (1-0.9990000000000001)) and alertmanager_notifications_failed:burnrate4d{job="alertmanager",slo="api-alerting-notif-availability-slo"}
        > (1 * (1-0.9990000000000001))
      for: 3h0m0s
      labels:
        exhaustion: 4w
        job: alertmanager
        long_burnrate_window: 4d
        service: rhobs.regional
        severity: warning
        short_burnrate_window: 6h
        slo: api-alerting-notif-availability-slo
  - interval: 30s
    name: api-alerting-notif-availability-slo-generic
    rules:
    - expr: "0.9990000000000001"
      labels:
        service: rhobs.regional
        slo: api-alerting-notif-availability-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        service: rhobs.regional
        slo: api-alerting-notif-availability-slo
      record: pyrra_window
    - expr: 1 - sum(alertmanager_notifications_failed:increase4w{code=~"^5..$",job="alertmanager",slo="api-alerting-notif-availability-slo"}
        or vector(0)) / sum(alertmanager_notifications_failed:increase4w{job="alertmanager",slo="api-alerting-notif-availability-slo"})
      labels:
        service: rhobs.regional
        slo: api-alerting-notif-availability-slo
      record: pyrra_availability
    - expr: sum(alertmanager_notifications_failed_total{job="alertmanager"})
      labels:
        service: rhobs.regional
        slo: api-alerting-notif-availability-slo
      record: pyrra_requests_total
    - expr: sum(alertmanager_notifications_failed_total{code=~"^5..$",job="alertmanager"}
        or vector(0))
      labels:
        service: rhobs.regional
        slo: api-alerting-notif-availability-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: api-metrics-write-latency-slo-increase
    rules:
    - expr: sum by (code) (increase(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api"}[4w]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-write-latency-slo
      record: http_request_duration_seconds:increase4w
    - expr: sum by (code) (increase(http_request_duration_seconds_bucket{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api",le="5"}[4w]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-api
        le: "5"
        service: rhobs.regional
        slo: api-metrics-write-latency-slo
      record: http_request_duration_seconds:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /receive handler is burning too much error budget to guarantee
          latency SLOs.
        message: API /receive handler is burning too much latency error budget.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteLatencyErrorBudgetBurning
      expr: absent(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api"})
        == 1
      for: 2m
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-api
        service: rhobs.regional
        severity: medium
        slo: api-metrics-write-latency-slo
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /receive handler is burning too much error budget to guarantee
          latency SLOs.
        message: API /receive handler is burning too much latency error budget.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteLatencyErrorBudgetBurning
      expr: absent(http_request_duration_seconds_bucket{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api",le="5"})
        == 1
      for: 2m
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-api
        service: rhobs.regional
        severity: medium
        slo: api-metrics-write-latency-slo
  - interval: 30s
    name: api-metrics-write-latency-slo
    rules:
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api"}[5m]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api",le="5"}[5m])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api"}[5m]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-write-latency-slo
      record: http_request_duration_seconds:burnrate5m
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api"}[30m]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api",le="5"}[30m])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api"}[30m]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-write-latency-slo
      record: http_request_duration_seconds:burnrate30m
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api"}[1h]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api",le="5"}[1h])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api"}[1h]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-write-latency-slo
      record: http_request_duration_seconds:burnrate1h
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api"}[2h]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api",le="5"}[2h])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api"}[2h]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-write-latency-slo
      record: http_request_duration_seconds:burnrate2h
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api"}[6h]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api",le="5"}[6h])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api"}[6h]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-write-latency-slo
      record: http_request_duration_seconds:burnrate6h
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api"}[1d]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api",le="5"}[1d])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api"}[1d]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-write-latency-slo
      record: http_request_duration_seconds:burnrate1d
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api"}[4d]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api",le="5"}[4d])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api"}[4d]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-api
        service: rhobs.regional
        slo: api-metrics-write-latency-slo
      record: http_request_duration_seconds:burnrate4d
    - alert: APIMetricsWriteLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /receive handler is burning too much error budget to guarantee
          latency SLOs.
        message: API /receive handler is burning too much latency error budget.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteLatencyErrorBudgetBurning
      expr: http_request_duration_seconds:burnrate5m{group="metricsv1",handler="receive",job="observatorium-api",slo="api-metrics-write-latency-slo"}
        > (14 * (1-0.9)) and http_request_duration_seconds:burnrate1h{group="metricsv1",handler="receive",job="observatorium-api",slo="api-metrics-write-latency-slo"}
        > (14 * (1-0.9))
      for: 2m
      labels:
        exhaustion: 2d
        group: metricsv1
        handler: receive
        job: observatorium-api
        long_burnrate_window: 1h
        service: rhobs.regional
        severity: critical
        short_burnrate_window: 5m
        slo: api-metrics-write-latency-slo
    - alert: APIMetricsWriteLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /receive handler is burning too much error budget to guarantee
          latency SLOs.
        message: API /receive handler is burning too much latency error budget.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteLatencyErrorBudgetBurning
      expr: http_request_duration_seconds:burnrate30m{group="metricsv1",handler="receive",job="observatorium-api",slo="api-metrics-write-latency-slo"}
        > (7 * (1-0.9)) and http_request_duration_seconds:burnrate6h{group="metricsv1",handler="receive",job="observatorium-api",slo="api-metrics-write-latency-slo"}
        > (7 * (1-0.9))
      for: 15m
      labels:
        exhaustion: 4d
        group: metricsv1
        handler: receive
        job: observatorium-api
        long_burnrate_window: 6h
        service: rhobs.regional
        severity: critical
        short_burnrate_window: 30m
        slo: api-metrics-write-latency-slo
    - alert: APIMetricsWriteLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /receive handler is burning too much error budget to guarantee
          latency SLOs.
        message: API /receive handler is burning too much latency error budget.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteLatencyErrorBudgetBurning
      expr: http_request_duration_seconds:burnrate2h{group="metricsv1",handler="receive",job="observatorium-api",slo="api-metrics-write-latency-slo"}
        > (2 * (1-0.9)) and http_request_duration_seconds:burnrate1d{group="metricsv1",handler="receive",job="observatorium-api",slo="api-metrics-write-latency-slo"}
        > (2 * (1-0.9))
      for: 1h
      labels:
        exhaustion: 2w
        group: metricsv1
        handler: receive
        job: observatorium-api
        long_burnrate_window: 1d
        service: rhobs.regional
        severity: warning
        short_burnrate_window: 2h
        slo: api-metrics-write-latency-slo
    - alert: APIMetricsWriteLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/283e7002d85c08126681241df2fdb22b/rhobs-next-slos?orgId=1&refresh=10s&var-datasource={{$externalLabels.cluster}}-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        description: API /receive handler is burning too much error budget to guarantee
          latency SLOs.
        message: API /receive handler is burning too much latency error budget.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteLatencyErrorBudgetBurning
      expr: http_request_duration_seconds:burnrate6h{group="metricsv1",handler="receive",job="observatorium-api",slo="api-metrics-write-latency-slo"}
        > (1 * (1-0.9)) and http_request_duration_seconds:burnrate4d{group="metricsv1",handler="receive",job="observatorium-api",slo="api-metrics-write-latency-slo"}
        > (1 * (1-0.9))
      for: 3h
      labels:
        exhaustion: 4w
        group: metricsv1
        handler: receive
        job: observatorium-api
        long_burnrate_window: 4d
        service: rhobs.regional
        severity: warning
        short_burnrate_window: 6h
        slo: api-metrics-write-latency-slo
  - interval: 30s
    name: api-metrics-write-latency-slo-generic
    rules:
    - expr: "0.9"
      labels:
        service: rhobs.regional
        slo: api-metrics-write-latency-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        service: rhobs.regional
        slo: api-metrics-write-latency-slo
      record: pyrra_window
    - expr: sum(http_request_duration_seconds:increase4w{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api",le="5",slo="api-metrics-write-latency-slo"}
        or vector(0)) / sum(http_request_duration_seconds:increase4w{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api",le="",slo="api-metrics-write-latency-slo"})
      labels:
        service: rhobs.regional
        slo: api-metrics-write-latency-slo
      record: pyrra_availability
    - expr: sum(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api"})
      labels:
        service: rhobs.regional
        slo: api-metrics-write-latency-slo
      record: pyrra_requests_total
    - expr: sum(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api"})
        - sum(http_request_duration_seconds_bucket{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-api",le="5"})
      labels:
        service: rhobs.regional
        slo: api-metrics-write-latency-slo
      record: pyrra_errors_total
